{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a626959-61c8-4bba-84d2-2a4ecab1f7ec",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# DLT pipeline\n",
    "\n",
    "This Delta Live Tables (DLT) definition is executed using a pipeline defined in resources/python_dlt.pipeline.yml."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9198e987-5606-403d-9f6d-8f14e6a4017f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "ename": "SparkConnectGrpcException",
     "evalue": "PERMISSION_DENIED: Cannot access Spark Connect. (traceId=562809b86e5d723f499652bbac58eeb5)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mSparkConnectGrpcException\u001b[39m                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m expr\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpython_dlt\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m main\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m spark = \u001b[43mDatabricksSession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuilder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m spark.conf.set(\u001b[33m\"\u001b[39m\u001b[33mspark.sql.session.localRelationCacheThreshold\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m64\u001b[39m * \u001b[32m1024\u001b[39m * \u001b[32m1024\u001b[39m)\n\u001b[32m     12\u001b[39m sys.path.append(\u001b[38;5;28mstr\u001b[39m(spark.conf.get(\u001b[33m\"\u001b[39m\u001b[33mbundle.sourcePath\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Code\\lake-in-house\\projects\\python_dlt\\.venv\\Lib\\site-packages\\databricks\\connect\\session.py:400\u001b[39m, in \u001b[36mDatabricksSession.Builder.getOrCreate\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    380\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgetOrCreate\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> SparkSession:\n\u001b[32m    381\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    382\u001b[39m \u001b[33;03m    Get an existing :class:SparkSession for the provided configuration or, if there is no\u001b[39;00m\n\u001b[32m    383\u001b[39m \u001b[33;03m     existing one, create a new one.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    398\u001b[39m \u001b[33;03m        queries to this spark session are executed remotely.\u001b[39;00m\n\u001b[32m    399\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Code\\lake-in-house\\projects\\python_dlt\\.venv\\Lib\\site-packages\\databricks\\connect\\session.py:518\u001b[39m, in \u001b[36mDatabricksSession.Builder._create\u001b[39m\u001b[34m(self, _Builder__skip_cache)\u001b[39m\n\u001b[32m    516\u001b[39m logger.debug(\u001b[33m\"\u001b[39m\u001b[33mFalling back to default configuration from the SDK.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    517\u001b[39m config = Config()\n\u001b[32m--> \u001b[39m\u001b[32m518\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_from_sdkconfig\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    519\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_gen_user_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    520\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_session_enabled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_env\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m__skip_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43m__skip_cache\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Code\\lake-in-house\\projects\\python_dlt\\.venv\\Lib\\site-packages\\databricks\\connect\\cache.py:59\u001b[39m, in \u001b[36mcached.<locals>._cached.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m __skip_cache \u001b[38;5;129;01mor\u001b[39;00m cache_id \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m cache \u001b[38;5;129;01mor\u001b[39;00m is_stale(cache[cache_id]):\n\u001b[32m     58\u001b[39m     logger.debug(\u001b[33m\"\u001b[39m\u001b[33mCaching: creating a new session.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     cache[cache_id] = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     61\u001b[39m     logger.debug(\u001b[33m\"\u001b[39m\u001b[33mCaching: reusing existing session.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Code\\lake-in-house\\projects\\python_dlt\\.venv\\Lib\\site-packages\\databricks\\connect\\session.py:624\u001b[39m, in \u001b[36mDatabricksSession.Builder._from_sdkconfig\u001b[39m\u001b[34m(config, user_agent, headers, validate_session, env, **kwargs)\u001b[39m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m validate_session \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m serverless_mode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mx-databricks-session-id\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m headers:\n\u001b[32m--> \u001b[39m\u001b[32m624\u001b[39m         \u001b[43mvalidate_session_serverless\u001b[49m\u001b[43m(\u001b[49m\u001b[43msession\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    625\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    626\u001b[39m         validate_session_with_sdk(config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Code\\lake-in-house\\projects\\python_dlt\\.venv\\Lib\\site-packages\\databricks\\connect\\validation.py:106\u001b[39m, in \u001b[36mvalidate_session_serverless\u001b[39m\u001b[34m(session)\u001b[39m\n\u001b[32m    102\u001b[39m logger.debug(\u001b[33m\"\u001b[39m\u001b[33mValidating serverless configuration.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    104\u001b[39m msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDatabricks Connect \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m__dbconnect_version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is unsupported with serverless. \u001b[39m\u001b[33m\"\u001b[39m \\\n\u001b[32m    105\u001b[39m       \u001b[33m\"\u001b[39m\u001b[33mUse with caution! \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m dbr_version_string = \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mserver_version\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.dbr_version\n\u001b[32m    107\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dbr_version_string:\n\u001b[32m    108\u001b[39m     \u001b[38;5;66;03m# dbr_version is introduced in DBR 16. If empty, the server is running DBR<16.\u001b[39;00m\n\u001b[32m    109\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(msg)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Code\\lake-in-house\\projects\\python_dlt\\.venv\\Lib\\site-packages\\pyspark\\sql\\connect\\client\\core.py:1436\u001b[39m, in \u001b[36mSparkConnectClient.server_version\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1435\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mserver_version\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> ServerVersion:\n\u001b[32m-> \u001b[39m\u001b[32m1436\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_analyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspark_version\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1437\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ServerVersion(result.spark_version, result.dbr_version)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Code\\lake-in-house\\projects\\python_dlt\\.venv\\Lib\\site-packages\\pyspark\\sql\\connect\\client\\core.py:1723\u001b[39m, in \u001b[36mSparkConnectClient._analyze\u001b[39m\u001b[34m(self, method, **kwargs)\u001b[39m\n\u001b[32m   1721\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m SparkConnectException(\u001b[33m\"\u001b[39m\u001b[33mInvalid state during retry exception handling.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1722\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[32m-> \u001b[39m\u001b[32m1723\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Code\\lake-in-house\\projects\\python_dlt\\.venv\\Lib\\site-packages\\pyspark\\sql\\connect\\client\\core.py:2244\u001b[39m, in \u001b[36mSparkConnectClient._handle_error\u001b[39m\u001b[34m(self, error)\u001b[39m\n\u001b[32m   2242\u001b[39m \u001b[38;5;28mself\u001b[39m.thread_local.inside_error_handling = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   2243\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(error, grpc.RpcError):\n\u001b[32m-> \u001b[39m\u001b[32m2244\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle_rpc_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2245\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(error, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[32m   2246\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mCannot invoke RPC\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(error) \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mclosed\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(error):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Code\\lake-in-house\\projects\\python_dlt\\.venv\\Lib\\site-packages\\pyspark\\sql\\connect\\client\\core.py:2355\u001b[39m, in \u001b[36mSparkConnectClient._handle_rpc_error\u001b[39m\u001b[34m(self, rpc_error)\u001b[39m\n\u001b[32m   2346\u001b[39m             \u001b[38;5;66;03m# END-EDGE\u001b[39;00m\n\u001b[32m   2348\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m convert_exception(\n\u001b[32m   2349\u001b[39m                 info,\n\u001b[32m   2350\u001b[39m                 status.message,\n\u001b[32m   2351\u001b[39m                 \u001b[38;5;28mself\u001b[39m._fetch_enriched_error(info),\n\u001b[32m   2352\u001b[39m                 \u001b[38;5;28mself\u001b[39m._display_server_stack_trace(),\n\u001b[32m   2353\u001b[39m             ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2355\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m SparkConnectGrpcException(status.message) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2356\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2357\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m SparkConnectGrpcException(\u001b[38;5;28mstr\u001b[39m(rpc_error)) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mSparkConnectGrpcException\u001b[39m: PERMISSION_DENIED: Cannot access Spark Connect. (traceId=562809b86e5d723f499652bbac58eeb5)"
     ]
    }
   ],
   "source": [
    "# Import DLT and src/python_dlt\n",
    "import dlt\n",
    "import sys\n",
    "from databricks.connect import DatabricksSession\n",
    "from pyspark.sql.functions import expr\n",
    "from python_dlt import main\n",
    "\n",
    "spark = DatabricksSession.builder.getOrCreate()\n",
    "spark.conf.set(\"spark.sql.session.localRelationCacheThreshold\", 64 * 1024 * 1024)\n",
    "\n",
    "\n",
    "sys.path.append(str(spark.conf.get(\"bundle.sourcePath\", \".\")))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3fc19dba-61fd-4a89-8f8c-24fee63bfb14",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@dlt.view\n",
    "def taxi_raw():\n",
    "    return main.get_taxis(spark)\n",
    "\n",
    "\n",
    "@dlt.table\n",
    "def filtered_taxis():\n",
    "    return dlt.read(\"taxi_raw\").filter(expr(\"fare_amount < 30\"))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "dlt_pipeline",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
